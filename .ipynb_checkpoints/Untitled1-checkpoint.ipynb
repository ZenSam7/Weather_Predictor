{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f3c7b5b-2f6a-4c7f-86c5-f8b75a266d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "import keras\n",
    "import os\n",
    "from keras.layers import Flatten, Dense, SimpleRNN, LSTM, BatchNormalization, Conv1D, Dropout\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import Weather_Data as WD\n",
    "from time import time\n",
    "\n",
    "# Убираем предупреждения\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "\n",
    "def what_device_use(device=\"cpu\"):\n",
    "    if device.lower() == \"gpu\":\n",
    "        # Работаем с GPU\n",
    "        tf.config.set_visible_devices(tf.config.list_physical_devices(\"GPU\"), \"GPU\")\n",
    "\n",
    "    if device.lower() == \"cpu\":\n",
    "        # Работаем с CPU\n",
    "        tf.config.set_visible_devices([], \"GPU\")\n",
    "\n",
    "\n",
    "def load_data(name_db=\"moscow\", len_test_data=0):\n",
    "    \"\"\"Загружаем данные\"\"\"\n",
    "    global train_data, train_data_answer, test_data, test_data_answer\n",
    "\n",
    "    # В WD.get_moscow_data     477_603 записей      (все данные идут с шагом в 1 часа)\n",
    "    # В WD.get_fresh_data        1_455 записей      (данные за последние 60 дней, идут с шагом в 1 час)\n",
    "\n",
    "    if name_db == \"moscow\":\n",
    "        DATA_out = WD.get_moscow_data()\n",
    "    elif name_db == \"fresh\":\n",
    "        DATA_out = WD.get_fresh_data()\n",
    "    print(\">>> Dataset loaded\\n\")\n",
    "\n",
    "\n",
    "    \"\"\"DATA_in == Данные погоды, начиная с 1ого дня (принимает)\n",
    "       DATA_out == Данные погоды, начиная с 2ого дня (должен предсказать)\"\"\"\n",
    "\n",
    "    # Создаём смещени назад во времени\n",
    "    DATA_in = DATA_out[:-1]\n",
    "    DATA_out = DATA_out[1:]\n",
    "\n",
    "    # Преобразуем данные\n",
    "    DATA_out = np.array(DATA_out).reshape((len(DATA_out), 1, 8))\n",
    "    DATA_in = np.array(DATA_in).reshape((len(DATA_out), 1, 8))\n",
    "\n",
    "    DATA_out = WD.normalize(DATA_out - DATA_in) # Остаточное обучение + нормализуем от -1 до 1 (а не от -0.01 до 0.01)\n",
    "    DATA_out = DATA_out[:, :, 3:]               # ИИшке не надо предсказывать время\n",
    "\n",
    "    # Разделяем часть для обучения и для тестирования\n",
    "    train_data = DATA_in[:-len_test_data] if len_test_data > 0 else DATA_in\n",
    "    train_data_answer = DATA_out[:-len_test_data] if len_test_data > 0 else DATA_out\n",
    "\n",
    "    if len_test_data > 0:\n",
    "        test_data = DATA_in[-len_test_data:]\n",
    "        test_data_answer = DATA_out[-len_test_data:, 0, :]\n",
    "    else:\n",
    "        test_data, test_data_answer = [], []\n",
    "\n",
    "\n",
    "def create_ai(num_layers_conv=3, num_ai_layers=5, num_neurons=32, print_summary=True):\n",
    "    \"\"\"Создаём ИИшки\"\"\"\n",
    "    global ai\n",
    "    # Суть в том, чтобы расперелить задачи по предсказыванию между разными нейронками\n",
    "    # Т.к. одна нейросеть очень плохо предскаывает одновременно все факторы\n",
    "\n",
    "    # У всех нейронок одна архитектура и один вход\n",
    "    input_layer = keras.Input((1, 8))\n",
    "\n",
    "\n",
    "    class Architecture:\n",
    "        def get_ai(self):\n",
    "            num_conv_neurons = 8\n",
    "            # Добавляем нормализаци, т.к. некоторый значения больше склонны бать с одной стороны (если взять среднее)\n",
    "            # (например температура чаще положительная, чем отрицательная (в москве))\n",
    "            list_layers = []\n",
    "\n",
    "            # Добавляем Conv1D\n",
    "            for _ in range(num_layers_conv):\n",
    "                list_layers.append(Conv1D(num_conv_neurons, 8, padding=\"same\"))\n",
    "                num_conv_neurons *= 2\n",
    "\n",
    "            # Добавляем остальные слои\n",
    "            for i in range(num_ai_layers):\n",
    "                if i % 2 == 0:\n",
    "                    list_layers.append(LSTM(num_neurons, return_sequences=True, unroll=True))\n",
    "                else:\n",
    "                    list_layers.append(Dense(num_neurons, activation=\"relu\"))\n",
    "\n",
    "            return Sequential(list_layers)(input_layer)\n",
    "\n",
    "    # Создаём 5 полностью независимые нейронки\n",
    "    temperature = Dense(1, activation=\"tanh\", name=\"temp\")(Architecture().get_ai())\n",
    "    pressure = Dense(1, activation=\"tanh\", name=\"press\")(Architecture().get_ai())\n",
    "    humidity = Dense(1, activation=\"tanh\", name=\"humid\")(Architecture().get_ai())\n",
    "    cloud = Dense(1, activation=\"tanh\", name=\"cloud\")(Architecture().get_ai())\n",
    "    rain = Dense(1, activation=\"tanh\", name=\"rain\")(Architecture().get_ai())\n",
    "\n",
    "\n",
    "    ai = keras.Model(input_layer, [temperature, pressure, humidity, cloud, rain], name=\"Weather_Predictor\")\n",
    "    ai.compile(optimizer=keras.optimizers.Adam(1e-3), loss=\"mean_squared_error\",\n",
    "               loss_weights={\"temp\": 100_000, \"press\": 10_000, \"humid\": 10_000, \"cloud\": 10_000, \"rain\": 100_000},)\n",
    "               # Отдаём приоритет температуре и осадкам, и увеличиваем ошибки (иначе они будут ≈0)\n",
    "\n",
    "    if print_summary:\n",
    "        ai.summary(); print()\n",
    "\n",
    "\n",
    "def ai_name(name):\n",
    "    \"\"\"Сохранения / Загрузки\"\"\"\n",
    "    global save_path, SAVE_NAME\n",
    "\n",
    "    save_path = lambda ai_name: f\"Saves Weather Prophet/{ai_name}\"\n",
    "    SAVE_NAME = lambda num: f\"{name}~{num}\"\n",
    "\n",
    "\n",
    "def load_ai(loading_with_learning_cycle=-1, print_summary=False):\n",
    "    \"\"\"ЗАГРУЖАЕМСЯ\"\"\"\n",
    "    global ai\n",
    "\n",
    "    # Вычисляем номер последнего сохранения с текущем именем\n",
    "    if loading_with_learning_cycle == -1:\n",
    "        loading_with_learning_cycle = int(sorted([save_name if SAVE_NAME(0)[:-2] in save_name\n",
    "                    else None for save_name in os.listdir(\"Saves Weather Prophet\")])[-1].split(\"~\")[-1])\n",
    "\n",
    "    print(f\">>> Loading the {SAVE_NAME(loading_with_learning_cycle)}\", end=\"\\t\\t\")\n",
    "    ai = tf.keras.models.load_model(save_path(SAVE_NAME(loading_with_learning_cycle)))\n",
    "    print(\"Done\\n\")\n",
    "\n",
    "    if print_summary:\n",
    "        ai.summary(); print()\n",
    "\n",
    "\n",
    "\n",
    "def train_ai(start_on=-1, finish_on=99, # Начинаем с номера последнего сохранения до finish_on\n",
    "             save_every_learning_cycle=True,    # Сохранять ли каждую ИИшку\n",
    "             epochs=3, batch_size=100,  verbose=2, # Параметры fit()\n",
    "             print_ai_answers=True, len_prints_ai_answers=100, # Выводить и сравнивать данные, или нет\n",
    "             print_weather_predict=True, len_predict_days=3, # Выводить ли  прогноз погоды\n",
    "             use_callbacks=False, callbacks_min_delta=10, callbacks_patience=3, # Параметры callbacks\n",
    "             shift_dataset_every_cycle=True, start_with_dataset_offset=0, # Смещаем данные на 1 час каждый цикл\n",
    "                     # (т.е. после первого смещения ИИшка должна предсказывать на 2 часа вперёд, потом на 3...)\n",
    "             ):\n",
    "    \"\"\"Обучение\"\"\"\n",
    "    global train_data, train_data_answer, test_data, test_data_answer\n",
    "    num_dataset_offset = 1\n",
    "\n",
    "    # Сдвигаемм наборы данных\n",
    "    if start_with_dataset_offset > 0:\n",
    "        num_dataset_offset += start_with_dataset_offset\n",
    "        train_data = train_data[: -start_with_dataset_offset]\n",
    "        train_data_answer = train_data_answer[start_with_dataset_offset:]\n",
    "        if len(train_data) > 0:\n",
    "            test_data = test_data[: -start_with_dataset_offset]\n",
    "            test_data_answer = test_data_answer[start_with_dataset_offset:]\n",
    "\n",
    "\n",
    "    callbacks = [keras.callbacks.EarlyStopping(monitor=\"loss\",\n",
    "                min_delta=callbacks_min_delta, patience=callbacks_patience, verbose=False)] \\\n",
    "        if use_callbacks else None\n",
    "\n",
    "    # Продолжаем с последнего сохранения если start_on == -1 (или создаём новое)\n",
    "    if start_on == -1:\n",
    "        try:\n",
    "            start_on = int(sorted([save_name if SAVE_NAME(0)[:-2] in save_name else None\n",
    "                            for save_name in os.listdir(\"Saves Weather Prophet\")])[-1].split(\"~\")[-1])\n",
    "        except:\n",
    "            start_on = 0\n",
    "\n",
    "    # Циклы обучения\n",
    "    for learning_cycle in range(start_on, finish_on):\n",
    "        print(f\">>> Learning the {SAVE_NAME(learning_cycle)}\\t\\t\\tПредсказывает на: {num_dataset_offset} ч вперёд\")\n",
    "        ai.fit(train_data, train_data_answer,\n",
    "               epochs=epochs, batch_size=batch_size,\n",
    "               verbose=verbose, shuffle=False, callbacks=callbacks)\n",
    "        print()\n",
    "\n",
    "\n",
    "        # Сохраняем\n",
    "        if save_every_learning_cycle:\n",
    "            print(f\">>> Saving the {SAVE_NAME(learning_cycle)}  (Ignore the WARNING)\", end=\"\\t\\t\")\n",
    "            ai.save(save_path(SAVE_NAME(learning_cycle)))\n",
    "            print(\"Done\\n\")\n",
    "\n",
    "\n",
    "        # Выводим данные и сравниваем\n",
    "        if print_ai_answers:\n",
    "            # Используем train_data если test_data нет\n",
    "            WD.print_ai_answers(ai, test_data if len(test_data)>0 else train_data, len_prints_ai_answers)\n",
    "        if print_weather_predict:\n",
    "            WD.print_weather_predict(ai, len_predict_days)\n",
    "\n",
    "\n",
    "        # Создаём смещение данных на 1 час\n",
    "        if shift_dataset_every_cycle:\n",
    "            num_dataset_offset += 1\n",
    "            train_data = train_data[: -1]\n",
    "            train_data_answer = train_data_answer[1:]\n",
    "            if len(train_data) > 0:\n",
    "                test_data = test_data[: -1]\n",
    "                test_data_answer = test_data_answer[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cc5d61-dde5-4a62-96b5-c65f98b6ae4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "what_device_use(\"gpu\")\n",
    "ai_name(\"AI_v4.2\")\n",
    "load_data(\"moscow\", len_test_data=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd50369-5684-44b1-a86c-ce8ac99f4888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_ai(5, 5, 128, print_summary=True)\n",
    "load_ai(-1, print_summary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41afb7c5-9a14-437f-8f50-0384ee2b0ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "WD.print_weather_predict(ai, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3ff887-e624-4a0e-8c09-8ad4183f6e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ai(epochs=1, batch_size=500, verbose=1, start_with_dataset_offset=0,\n",
    "         shift_dataset_every_cycle=False, amount_available_context=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
