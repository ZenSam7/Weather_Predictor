{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-12T05:03:23.021870800Z",
     "start_time": "2025-07-12T05:03:20.093723100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Importing libraries\t\tDone\n"
     ]
    }
   ],
   "source": [
    "print(\">>> Importing libraries\", end=\"\\t\\t\")\n",
    "\n",
    "import numpy as np\n",
    "from keras.layers import (\n",
    "    Flatten,\n",
    "    Dense,\n",
    "    SimpleRNN,\n",
    "    LSTM,\n",
    "    BatchNormalization,\n",
    "    Conv1D,\n",
    "    Dropout,\n",
    "    Input,\n",
    ")\n",
    "import keras\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from logging import ERROR\n",
    "import Weather_Data as wd\n",
    "\n",
    "tf.data.experimental.enable_debug_mode()\n",
    "\n",
    "# Убираем предупреждения\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "tf.get_logger().setLevel(ERROR)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def what_device_use(device=\"cpu\"):\n",
    "    # Работаем с CPU\n",
    "    tf.config.set_visible_devices([], \"GPU\")\n",
    "\n",
    "    if device.lower() == \"gpu\":\n",
    "        # Работаем с GPU\n",
    "        tf.config.set_visible_devices(tf.config.list_physical_devices(\"GPU\"), \"GPU\")\n",
    "\n",
    "\n",
    "def load_data(name_db=\"moscow\", how_many_context_days=20):\n",
    "    \"\"\"Загружаем данные\"\"\"\n",
    "    global train_data, train_data_answer\n",
    "    print(\"\\n>>> Loading and processing Dataset\", end=\"\\t\\t\")\n",
    "\n",
    "    # В wd.get_moscow_data     167_217 записей      (все данные идут с шагом в 3 часа)\n",
    "    # В wd.get_fresh_data        1_455 записей      (данные за последние 60 дней, идут с шагом в 3 часа)\n",
    "\n",
    "    DATA = wd.get_moscow_data()\n",
    "    if name_db == \"fresh\":\n",
    "        DATA = wd.get_fresh_data(how_many_context_days)\n",
    "\n",
    "    \"\"\"DATA_in == Данные погоды, начиная с 1й записи (принимает)\n",
    "       DATA_out == Данные погоды, начиная с 2й записи (должен предсказать)\"\"\"\n",
    "\n",
    "    # Создаём смещени назад во времени\n",
    "    DATA_in = DATA[:-1]\n",
    "    DATA_out = DATA[1:]\n",
    "\n",
    "    # Преобразуем данные\n",
    "    DATA_in = np.array(DATA_in).reshape((len(DATA_out), 1, 8))\n",
    "    DATA_out = np.array(DATA_out).reshape((len(DATA_out), 1, 8))\n",
    "\n",
    "    # Остаточное обучение\n",
    "    DATA_out = DATA_in - DATA_out\n",
    "    # ИИшке не надо предсказывать время\n",
    "    DATA_out = DATA_out[:, :, 3:]\n",
    "    # Нормализуем (чтобы ИИшка могла как можно шире )\n",
    "    # DATA_out = wd.normalize(DATA_out)\n",
    "\n",
    "    train_data = DATA_in\n",
    "    train_data_answer = DATA_out\n",
    "\n",
    "    print(\"Done\\n\")\n",
    "\n",
    "\n",
    "def ai_name(name):\n",
    "    \"\"\"Всякие функции\"\"\"\n",
    "\n",
    "    global get_save_path, get_save_name, get_start_with, save_ai, load_ai\n",
    "\n",
    "    def get_save_path(ai_name):\n",
    "        return f\"./Saves_Weather_Prophet/{ai_name}\"\n",
    "\n",
    "    def get_save_name(num):\n",
    "        return f\"{name}~{num}\"\n",
    "\n",
    "    def save_ai(num):\n",
    "        print(f\"\\n>>> Saving the {get_save_name(num)}  (Ignore the WARNING)\", end=\"\\t\\t\")\n",
    "        ai.save(get_save_path(get_save_name(num)))\n",
    "        print(\"Done\\n\")\n",
    "\n",
    "    def get_start_with(start_with=-1):\n",
    "        # Вычисляем номер последнего сохранения с текущем именем\n",
    "        if start_with == -1:\n",
    "            try:\n",
    "                saves = []\n",
    "                for save_name in os.listdir(\"Saves_Weather_Prophet\"):\n",
    "                    if get_save_name(0)[:-2] in save_name:\n",
    "                        saves.append(save_name)\n",
    "\n",
    "                assert saves != [], f\"Нет ни одного сохранения с именем {get_save_name(0)[:-2]}\"\n",
    "\n",
    "                start_with = int(sorted(saves)[-1].split(\"~\")[-1])\n",
    "            except BaseException:\n",
    "                return 0\n",
    "\n",
    "        return start_with\n",
    "\n",
    "    def load_ai(load_with=-1, print_summary=False):\n",
    "        \"\"\"ЗАГРУЖАЕМСЯ\"\"\"\n",
    "        global ai\n",
    "\n",
    "        # Вычисляем номер последнего сохранения с текущем именем\n",
    "        loading_with = get_start_with(load_with)\n",
    "\n",
    "        print(f\">>> Loading the {get_save_name(loading_with)}\", end=\"\\t\\t\")\n",
    "        ai = tf.keras.models.load_model(get_save_path(get_save_name(loading_with)))\n",
    "        print(\"Done\\n\")\n",
    "        if print_summary:\n",
    "            ai.summary()\n",
    "            print()\n",
    "\n",
    "\n",
    "def show_architecture_ai(ai):\n",
    "    from keras.utils.vis_utils import plot_model\n",
    "\n",
    "    name = str(get_save_name(0))[:-2]\n",
    "    plot_model(ai, to_file=f\"{name}.png\", show_shapes=True, show_layer_names=True)\n",
    "\n",
    "\n",
    "def create_ai(\n",
    "    num_layers_conv=3,\n",
    "    num_main_layers=5,\n",
    "    num_neurons=32,\n",
    "    batch_size=100,\n",
    "    print_summary=True,\n",
    "):\n",
    "    \"\"\"Создаём ИИшки\"\"\"\n",
    "    global ai\n",
    "\n",
    "    # Суть в том, чтобы расперелить задачи по предсказыванию между разными нейронками\n",
    "    # Т.к. одна нейросеть очень плохо предскаывает одновременно все факторы\n",
    "    general_input = Input(batch_input_shape=(batch_size, 1, 8))\n",
    "\n",
    "    class Create_AI:\n",
    "        def get_model(self):\n",
    "            num_conv_neurons = 4\n",
    "            model = keras.Sequential()\n",
    "            model.add(general_input)\n",
    "\n",
    "            # Добавляем Conv1D\n",
    "            for _ in range(num_layers_conv):\n",
    "                num_conv_neurons *= 2\n",
    "                list_layers.append(Conv1D(num_conv_neurons, 8, padding=\"same\"))\n",
    "\n",
    "            # Добавляем основные слои (чередуем Dense и LSTM)\n",
    "            for i in range(num_main_layers):\n",
    "                list_layers.append(\n",
    "                    LSTM(\n",
    "                        num_neurons,\n",
    "                        activation=\"tanh\",\n",
    "                        return_sequences=True,\n",
    "                        unroll=False,\n",
    "                        stateful=True,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            return keras.Sequential(list_layers)(general_input)\n",
    "\n",
    "    # Создаём 5 полностью независимые нейронки\n",
    "    temperature = Dense(1, activation=\"tanh\", name=\"temp\")(Create_AI().get_model())\n",
    "    pressure = Dense(1, activation=\"tanh\", name=\"press\")(Create_AI().get_model())\n",
    "    humidity = Dense(1, activation=\"tanh\", name=\"humid\")(Create_AI().get_model())\n",
    "    cloud = Dense(1, activation=\"tanh\", name=\"cloud\")(Create_AI().get_model())\n",
    "    rain = Dense(1, activation=\"tanh\", name=\"rain\")(Create_AI().get_model())\n",
    "\n",
    "    ai = keras.Model(\n",
    "        general_input,\n",
    "        [temperature, pressure, humidity, cloud, rain],\n",
    "        # temperature,\n",
    "        name=\"Weather_Predictor\",\n",
    "    )\n",
    "\n",
    "    # mean_absolute_percentage_error\n",
    "    ai.compile(\n",
    "        optimizer=keras.optimizers.Adam(1e-3),\n",
    "        loss=\"mean_absolute_error\",\n",
    "        loss_weights={\n",
    "            \"temp\": 1_000,\n",
    "            \"press\": 100,\n",
    "            \"humid\": 100,\n",
    "            \"cloud\": 100,\n",
    "            \"rain\": 100,\n",
    "        },\n",
    "    )\n",
    "    # Отдаём приоритет температуре, и увеличиваем всем ошибки (иначе они будут ≈0)\n",
    "\n",
    "    if print_summary:\n",
    "        ai.summary()\n",
    "        print()\n",
    "\n",
    "def train_make_predict(\n",
    "    batch_size=100,\n",
    "    amount_batches=10,\n",
    "    len_predict=24,\n",
    "    start=-1,\n",
    "    finish_on=99,\n",
    "    increased_error_factor=100,\n",
    "):\n",
    "    \"\"\"Эта функция нужна чтобы обучить ИИшку состовлять прогноз\"\"\"\n",
    "\n",
    "    assert len_predict < batch_size, \"len_predict sould be < batch_size\"\n",
    "\n",
    "    tf.config.run_functions_eagerly(True)\n",
    "    ai.reset_states()  # Очищаем данные, оставшиеся после обучения\n",
    "\n",
    "    # Продолжаем с последнего сохранения если start_on == -1 (или создаём новое)\n",
    "    start_with = get_start_with(start) +1\n",
    "\n",
    "    # Циклы обучения\n",
    "    for learning_cycle in range(start_with, finish_on):\n",
    "        print(f\">>> Learning the {get_save_name(learning_cycle)}\")\n",
    "        losses = []\n",
    "\n",
    "        # Разделяем train_data на батчи (В посленем батче — ненужные данные)\n",
    "        batchs_data = [\n",
    "            train_data[i: i + batch_size]\n",
    "            for i in range(0, len(train_data), batch_size)\n",
    "        ][:-1]\n",
    "        # Берём рандомный промежуток батчей\n",
    "        rand = np.random.randint(len(batchs_data) - amount_batches)\n",
    "        batchs_data = batchs_data[:-1][rand: rand + amount_batches]\n",
    "\n",
    "        for b in tqdm(\n",
    "            range(len(batchs_data)), desc=f\"Epoch {learning_cycle}/{finish_on}\"\n",
    "        ):\n",
    "            times = tf.Variable(batchs_data[b][:, 0, :3], tf.float64)\n",
    "            data_batch = tf.Variable(batchs_data[b][:, 0, 3:], tf.float64)\n",
    "\n",
    "            losses.append(\n",
    "                train_step(times, data_batch, len_predict, increased_error_factor)\n",
    "            )\n",
    "\n",
    "        print(\n",
    "            f\"Loss: {round(np.mean(losses), 5)} (mean); {round(np.min(losses), 5)} min\\n\"\n",
    "        )\n",
    "\n",
    "        # Сохраняем\n",
    "        sane_ai(learning_cycle)\n",
    "\n",
    "        wd.print_weather_predict(ai, 1)\n",
    "\n",
    "\n",
    "def start_train(\n",
    "    start_on=-1,\n",
    "    finish_on=99,  # Начинаем с номера последнего сохранения до finish_on\n",
    "    epochs=3,\n",
    "    batch_size=100,\n",
    "    verbose=1,\n",
    "    print_ai_answers=True,\n",
    "    len_prints_ai_answers=100,\n",
    "    print_weather_predict=True,\n",
    "    len_predict_days=3,\n",
    "    use_callbacks=False,\n",
    "    callbacks_min_delta=10,\n",
    "    callbacks_patience=3,\n",
    "    save_model=True,\n",
    "):\n",
    "    \"\"\"Это просто большая обёртка вокруг функции обучения\"\"\"\n",
    "    global train_data, train_data_answer, ai\n",
    "\n",
    "    callbacks = (\n",
    "        [\n",
    "            keras.callbacks.EarlyStopping(\n",
    "                monitor=\"loss\",\n",
    "                min_delta=callbacks_min_delta,\n",
    "                patience=callbacks_patience,\n",
    "                verbose=False,\n",
    "            )\n",
    "        ]\n",
    "        if use_callbacks\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    # Продолжаем с последнего сохранения если start_on == -1 (или создаём новое)\n",
    "    start_with = get_start_with(start_on) +1\n",
    "\n",
    "    # Убираем немного записей, чтобы train_data можно было ровно разделить на batch_size\n",
    "    train_data = train_data[: len(train_data) // batch_size * batch_size]\n",
    "    train_data_answer = train_data_answer[: len(train_data) // batch_size * batch_size]\n",
    "\n",
    "    # Циклы обучения\n",
    "    for learning_cycle in range(start_with, finish_on):\n",
    "        print(f\">>> Learning the {get_save_name(learning_cycle)}\")\n",
    "\n",
    "        ai.reset_states()  # Очищаем данные, оставшиеся от обучения\n",
    "\n",
    "        ai.fit(\n",
    "            train_data,\n",
    "            train_data_answer,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            verbose=verbose,\n",
    "            shuffle=False,\n",
    "            callbacks=callbacks,\n",
    "        )\n",
    "\n",
    "        # Сохраняем\n",
    "        if save_model:\n",
    "            save_ai(learning_cycle)\n",
    "\n",
    "        # Выводим данные и сравниваем\n",
    "        if print_ai_answers:\n",
    "            wd.print_ai_answers(ai, train_data, batch_size, len_prints_ai_answers)\n",
    "        if print_weather_predict:\n",
    "            wd.print_weather_predict(ai, len_predict_days, batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T08:38:15.851005600Z",
     "start_time": "2024-04-05T08:38:15.843006800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Loading and processing Dataset\t\tDone\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'list_layers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 8\u001B[0m\n\u001B[0;32m      4\u001B[0m load_data(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmoscow\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      6\u001B[0m batch_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m128\u001B[39m\n\u001B[1;32m----> 8\u001B[0m \u001B[43mcreate_ai\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m7\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m128\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      9\u001B[0m load_ai(print_summary\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     11\u001B[0m start_train(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m, epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4\u001B[39m,\n\u001B[0;32m     12\u001B[0m     batch_size\u001B[38;5;241m=\u001B[39mbatch_size,\n\u001B[0;32m     13\u001B[0m     print_weather_predict\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m     14\u001B[0m     print_ai_answers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m     15\u001B[0m )\n",
      "Cell \u001B[1;32mIn[4], line 141\u001B[0m, in \u001B[0;36mcreate_ai\u001B[1;34m(num_layers_conv, num_main_layers, num_neurons, batch_size, print_summary)\u001B[0m\n\u001B[0;32m    138\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m keras\u001B[38;5;241m.\u001B[39mSequential(list_layers)(general_input)\n\u001B[0;32m    140\u001B[0m \u001B[38;5;66;03m# Создаём 5 полностью независимые нейронки\u001B[39;00m\n\u001B[1;32m--> 141\u001B[0m temperature \u001B[38;5;241m=\u001B[39m Dense(\u001B[38;5;241m1\u001B[39m, activation\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtanh\u001B[39m\u001B[38;5;124m\"\u001B[39m, name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtemp\u001B[39m\u001B[38;5;124m\"\u001B[39m)(\u001B[43mCreate_AI\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m    142\u001B[0m pressure \u001B[38;5;241m=\u001B[39m Dense(\u001B[38;5;241m1\u001B[39m, activation\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtanh\u001B[39m\u001B[38;5;124m\"\u001B[39m, name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpress\u001B[39m\u001B[38;5;124m\"\u001B[39m)(Create_AI()\u001B[38;5;241m.\u001B[39mget_model())\n\u001B[0;32m    143\u001B[0m humidity \u001B[38;5;241m=\u001B[39m Dense(\u001B[38;5;241m1\u001B[39m, activation\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtanh\u001B[39m\u001B[38;5;124m\"\u001B[39m, name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhumid\u001B[39m\u001B[38;5;124m\"\u001B[39m)(Create_AI()\u001B[38;5;241m.\u001B[39mget_model())\n",
      "Cell \u001B[1;32mIn[4], line 128\u001B[0m, in \u001B[0;36mcreate_ai.<locals>.Create_AI.get_model\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    126\u001B[0m \u001B[38;5;66;03m# Добавляем основные слои (чередуем Dense и LSTM)\u001B[39;00m\n\u001B[0;32m    127\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_main_layers):\n\u001B[1;32m--> 128\u001B[0m     \u001B[43mlist_layers\u001B[49m\u001B[38;5;241m.\u001B[39mappend(\n\u001B[0;32m    129\u001B[0m         LSTM(\n\u001B[0;32m    130\u001B[0m             num_neurons,\n\u001B[0;32m    131\u001B[0m             activation\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtanh\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    132\u001B[0m             return_sequences\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    133\u001B[0m             unroll\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    134\u001B[0m             stateful\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    135\u001B[0m         )\n\u001B[0;32m    136\u001B[0m     )\n\u001B[0;32m    138\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m keras\u001B[38;5;241m.\u001B[39mSequential(list_layers)(general_input)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'list_layers' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    what_device_use(\"cpu\")\n",
    "    ai_name(\"AI_v7.0\")\n",
    "    load_data(\"moscow\")\n",
    "\n",
    "    batch_size = 128\n",
    "\n",
    "    create_ai(0, 7, 128, batch_size)\n",
    "    load_ai(print_summary=True)\n",
    "\n",
    "    start_train(-1, 2, epochs=4,\n",
    "        batch_size=batch_size,\n",
    "        print_weather_predict=False,\n",
    "        print_ai_answers=True,\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T08:38:17.641193100Z",
     "start_time": "2024-04-05T08:38:16.001194700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
