{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "import keras\n",
    "import os\n",
    "from keras.layers import Flatten, Dense, SimpleRNN, LSTM, BatchNormalization, Conv1D, Dropout\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import Weather_Data as WD\n",
    "from time import time\n",
    "\n",
    "# Убираем предупреждения\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def what_device_use(device=\"cpu\"):\n",
    "    if device.lower() == \"gpu\":\n",
    "        # Работаем с GPU\n",
    "        tf.config.set_visible_devices(tf.config.list_physical_devices(\"GPU\"), \"GPU\")\n",
    "\n",
    "    if device.lower() == \"cpu\":\n",
    "        # Работаем с CPU\n",
    "        tf.config.set_visible_devices([], \"GPU\")\n",
    "\n",
    "def load_data(name_db=\"moscow\"):\n",
    "    \"\"\"Загружаем данные\"\"\"\n",
    "    global train_data, train_data_answer\n",
    "\n",
    "    # В WD.get_moscow_data     477_603 записей      (все данные идут с шагом в 1 часа)\n",
    "    # В WD.get_fresh_data        1_455 записей      (данные за последние 60 дней, идут с шагом в 1 час)\n",
    "\n",
    "    if name_db == \"moscow\":\n",
    "        DATA_out = WD.get_moscow_data()\n",
    "    elif name_db == \"fresh\":\n",
    "        DATA_out = WD.get_fresh_data()\n",
    "    print(\">>> Dataset loaded\\n\")\n",
    "\n",
    "\n",
    "    \"\"\"DATA_in == Данные погоды, начиная с 1ого дня (принимает)\n",
    "       DATA_out == Данные погоды, начиная с 2ого дня (должен предсказать)\"\"\"\n",
    "\n",
    "    # Создаём смещени назад во времени\n",
    "    DATA_in = DATA_out[:-1]\n",
    "    DATA_out = DATA_out[1:]\n",
    "\n",
    "    # Преобразуем данные\n",
    "    DATA_out = np.array(DATA_out).reshape((len(DATA_out), 1, 8))\n",
    "    DATA_in = np.array(DATA_in).reshape((len(DATA_out), 1, 8))\n",
    "\n",
    "    DATA_out = WD.normalize(DATA_out - DATA_in) # Остаточное обучение + нормализуем от -1 до 1 (а не от -0.01 до 0.01)\n",
    "    DATA_out = DATA_out[:, :, 3:]               # ИИшке не надо предсказывать время\n",
    "\n",
    "\n",
    "    train_data = DATA_in\n",
    "    train_data_answer = DATA_out\n",
    "\n",
    "def ai_name(name):\n",
    "    \"\"\"Сохранения / Загрузки\"\"\"\n",
    "    global save_path, SAVE_NAME\n",
    "\n",
    "    save_path = lambda ai_name: f\"Saves Weather Prophet/{ai_name}\"\n",
    "    SAVE_NAME = lambda num: f\"{name}~{num}\"\n",
    "\n",
    "def load_ai(loading_with_learning_cycle=-1, print_summary=False):\n",
    "    \"\"\"ЗАГРУЖАЕМСЯ\"\"\"\n",
    "    global ai\n",
    "\n",
    "    # Вычисляем номер последнего сохранения с текущем именем\n",
    "    if loading_with_learning_cycle == -1:\n",
    "        loading_with_learning_cycle = int(sorted([save_name if SAVE_NAME(0)[:-2] in save_name\n",
    "                    else None for save_name in os.listdir(\"Saves Weather Prophet\")])[-1].split(\"~\")[-1])\n",
    "\n",
    "    print(f\">>> Loading the {SAVE_NAME(loading_with_learning_cycle)}\", end=\"\\t\\t\")\n",
    "    ai = tf.keras.models.load_model(save_path(SAVE_NAME(loading_with_learning_cycle)))\n",
    "    print(\"Done\\n\")\n",
    "\n",
    "    if print_summary:\n",
    "        ai.summary(); print()\n",
    "\n",
    "\n",
    "def create_ai(num_layers_conv=3, num_ai_layers=5, num_neurons=32, print_summary=True):\n",
    "    \"\"\"Создаём ИИшки\"\"\"\n",
    "    global ai, loss_func, optimizer\n",
    "    # Суть в том, чтобы расперелить задачи по предсказыванию между разными нейронками\n",
    "    # Т.к. одна нейросеть очень плохо предскаывает одновременно все факторы\n",
    "\n",
    "    # У всех нейронок одна архитектура и один вход\n",
    "    input_layer = keras.Input((1, 8))\n",
    "\n",
    "    class Architecture:\n",
    "        def get_ai(self):\n",
    "            num_conv_neurons = 8\n",
    "            # Добавляем нормализаци, т.к. некоторый значения больше склонны бать с одной стороны (если взять среднее)\n",
    "            # (например температура чаще положительная, чем отрицательная (в москве))\n",
    "            list_layers = []\n",
    "\n",
    "            # Добавляем Conv1D\n",
    "            for _ in range(num_layers_conv):\n",
    "                list_layers.append(Conv1D(num_conv_neurons, 8, padding=\"same\"))\n",
    "                num_conv_neurons *= 2\n",
    "\n",
    "            # Добавляем остальные слои\n",
    "            for i in range(num_ai_layers):\n",
    "                if i % 2 == 0:\n",
    "                    list_layers.append(LSTM(num_neurons, return_sequences=True, unroll=True))\n",
    "                else:\n",
    "                    list_layers.append(Dense(num_neurons, activation=\"relu\"))\n",
    "\n",
    "            return Sequential(list_layers)(input_layer)\n",
    "\n",
    "    # Создаём 5 полностью независимые нейронки\n",
    "    temperature = Dense(1, activation=\"tanh\", name=\"temp\")(Architecture().get_ai())\n",
    "    pressure = Dense(1, activation=\"tanh\", name=\"press\")(Architecture().get_ai())\n",
    "    humidity = Dense(1, activation=\"tanh\", name=\"humid\")(Architecture().get_ai())\n",
    "    cloud = Dense(1, activation=\"tanh\", name=\"cloud\")(Architecture().get_ai())\n",
    "    rain = Dense(1, activation=\"tanh\", name=\"rain\")(Architecture().get_ai())\n",
    "\n",
    "    # ВРОДЕ БЫ не надо компилировать модель когда создаёшь свою функцию обучения\n",
    "    # (на счёт keras.Model я не знаю, убирать или нет но оно вроде \"собирает\" модель воедино)\n",
    "    ai = keras.Model(input_layer, [temperature, pressure, humidity, cloud, rain], name=\"Weather_Predictor\")\n",
    "\n",
    "    # ai.compile(optimizer=keras.optimizers.Adam(1e-3), loss=\"mean_squared_error\",\n",
    "    #            loss_weights={\"temp\": 100_000, \"press\": 10_000, \"humid\": 10_000, \"cloud\": 10_000, \"rain\": 100_000},)\n",
    "    #            # Отдаём приоритет температуре и осадкам, и увеличиваем ошибки (иначе они будут ≈0)\n",
    "\n",
    "    if print_summary:\n",
    "        ai.summary();\n",
    "        print()\n",
    "\n",
    "\n",
    "\n",
    "def start_train(# ЭТА ФУНКЦИЯ НУЖНА ЧТОБЫ ОБУЧТЬ ИИШКУ ПРЕДСКАЗВАТЬ СЛЕДУЮЩИЙ ЧАС\n",
    "             start_on=-1, finish_on=99, # Начинаем с номера последнего сохранения до finish_on\n",
    "             epochs=3, batch_size=100,  verbose=2, # Параметры fit()\n",
    "             print_ai_answers=True, len_prints_ai_answers=100, # Выводить и сравнивать данные, или нет\n",
    "             print_weather_predict=True, len_predict_days=3, # Выводить ли  прогноз погоды\n",
    "             use_callbacks=False, callbacks_min_delta=10, callbacks_patience=3, # Параметры callbacks\n",
    "             shift_dataset_every_cycle=True, start_with_dataset_offset=0, # Смещаем данные на 1 час каждый цикл\n",
    "                     # (т.е. после первого смещения ИИшка должна предсказывать на 2 часа вперёд, потом на 3...)\n",
    "             ):\n",
    "    \"\"\"Это просто большая обёртка вокруг функции обучения\"\"\"\n",
    "    global train_data, train_data_answer\n",
    "    num_dataset_offset = 1\n",
    "\n",
    "    # Сдвигаемм наборы данных\n",
    "    if start_with_dataset_offset > 0:\n",
    "        num_dataset_offset += start_with_dataset_offset\n",
    "        train_data = train_data[: -start_with_dataset_offset]\n",
    "        train_data_answer = train_data_answer[start_with_dataset_offset:]\n",
    "\n",
    "\n",
    "    callbacks = [keras.callbacks.EarlyStopping(monitor=\"loss\",\n",
    "                min_delta=callbacks_min_delta, patience=callbacks_patience, verbose=False)] \\\n",
    "        if use_callbacks else None\n",
    "\n",
    "    # Продолжаем с последнего сохранения если start_on == -1 (или создаём новое)\n",
    "    if start_on == -1:\n",
    "        try:\n",
    "            start_on = int(sorted([save_name if SAVE_NAME(0)[:-2] in save_name else None\n",
    "                            for save_name in os.listdir(\"Saves Weather Prophet\")])[-1].split(\"~\")[-1])\n",
    "        except:\n",
    "            start_on = 0\n",
    "\n",
    "    # Циклы обучения\n",
    "    for learning_cycle in range(start_on, finish_on):\n",
    "        print(f\">>> Learning the {SAVE_NAME(learning_cycle)}\\t\\t\\tПредсказывает на: {num_dataset_offset} ч вперёд\")\n",
    "        ai.fit(train_data, train_data_answer,\n",
    "               epochs=epochs, batch_size=batch_size,\n",
    "               verbose=verbose, shuffle=False, callbacks=callbacks)\n",
    "        print()\n",
    "\n",
    "\n",
    "        # Сохраняем\n",
    "        print(f\">>> Saving the {SAVE_NAME(learning_cycle)}  (Ignore the WARNING)\", end=\"\\t\\t\")\n",
    "        ai.save(save_path(SAVE_NAME(learning_cycle)))\n",
    "        print(\"Done\\n\")\n",
    "\n",
    "\n",
    "        # Выводим данные и сравниваем\n",
    "        if print_ai_answers:\n",
    "            WD.print_ai_answers(ai, train_data, len_prints_ai_answers)\n",
    "        if print_weather_predict:\n",
    "            WD.print_weather_predict(ai, len_predict_days)\n",
    "\n",
    "\n",
    "        # Создаём смещение данных на 1 час\n",
    "        if shift_dataset_every_cycle:\n",
    "            num_dataset_offset += 1\n",
    "            train_data = train_data[: -1]\n",
    "            train_data_answer = train_data_answer[1:]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#########################################################################################################"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "what_device_use(\"gpu\")\n",
    "ai_name(\"AI_v4.2\")\n",
    "load_data(\"moscow\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create_ai(5, 5, 128, print_summary=True)\n",
    "load_ai(-1, print_summary=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#########################################################################################################"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def train_step(Times, Data_batch, next_batch_of_data, len_predict_days):\n",
    "    \"\"\"Делаем свою функцию fit()\n",
    "    (Нужна она чтобы обучать ИИшку составлять прогноз, на основе своих данных)\"\"\"\n",
    "    global loss_func, optimizer\n",
    "\n",
    "    times = Times\n",
    "    data_batch = Data_batch\n",
    "\n",
    "    with tf.GradientTape() as ai_tape:\n",
    "        # Составляем прогноз длиной len_predict_days *24\n",
    "        for _ in range(len_predict_days *24):\n",
    "\n",
    "            # Заполняем joind_data первым значением, а потом к нему добавляем другие\n",
    "            joind_data = tf.expand_dims(tf.expand_dims(tf.concat([times[0], data_batch[0]], 0), axis=0), axis=0)\n",
    "            for i in range(1, data_batch.shape[0]):\n",
    "                joind_vector = tf.concat([times[i], data_batch[i]], 0)\n",
    "                joind_vector = tf.expand_dims(tf.expand_dims(joind_vector, axis=0), axis=0)\n",
    "                joind_data = tf.concat([joind_data, joind_vector], axis=0)\n",
    "\n",
    "            # Записываем предсказание ИИшки на следующий час\n",
    "            ai_ans = [ai(joind_data, training=True)[i][-1][0] for i in range(5)]\n",
    "            ai_ans = tf.cast(tf.reshape(ai_ans, [1, -1]), tf.float64)\n",
    "\n",
    "            data_batch = tf.concat([data_batch[1:], ai_ans], 0) # Смещаем прогноз\n",
    "\n",
    "            # Обновляем время\n",
    "            time = times[-1]\n",
    "\n",
    "            to_add = tf.constant([1/12, 1/15.5, 1/6], dtype=tf.float64)\n",
    "            where_to_add = tf.cast([True, time[0] > 1, time[1] > 1], tf.float64)\n",
    "            time += to_add * where_to_add\n",
    "            # time[0] += 1 / 12                          # Увеличиваем часы\n",
    "            # time[1] += 1 / 15.5 if time[0] > 1 else 0  # Увеличиваем день\n",
    "            # time[2] += 1 / 6 if time[1] > 1 else 0     # Увеличиваем месяц\n",
    "\n",
    "            # Следим, чтобы зачения не выходили за границы\n",
    "            overflow = tf.cast(time > 1, tf.float64)\n",
    "            not_overflow = tf.cast(time <= 1, tf.float64)\n",
    "            time = time * not_overflow + overflow * tf.constant([-1], dtype=tf.float64)\n",
    "            # time = [-1 if i > 1 else i for i in time]\n",
    "\n",
    "            times = tf.concat([times[1:], tf.reshape(time, [1, -1])], 0) # Смещаем прогноз\n",
    "\n",
    "        # ИИшка должна предсказать будущую погоду\n",
    "        real_ans = next_batch_of_data[:len_predict_days *24]\n",
    "        ai_pred  = data_batch[:len_predict_days *24]\n",
    "        loss = tf.keras.losses.mean_squared_error(real_ans, ai_pred)\n",
    "\n",
    "        # Состовляем градиенты\n",
    "        gradients = ai_tape.gradient(loss, ai.trainable_variables)\n",
    "\n",
    "        # Изменяем веса\n",
    "        (keras.optimizers.Adam(1e-4)).apply_gradients(zip(gradients, ai.trainable_variables))\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "def train_make_predict(batch_size=200, len_predict_days=1):\n",
    "    # ЭТА ФУНКЦИЯ НУЖНА ЧТОБЫ ОБУЧИТЬ ИИШКУ СОСТОВЛЯТЬ ПРОГНОЗ\n",
    "    global train_data\n",
    "\n",
    "    # Разделяем train_data на батчи\n",
    "    batchs_data = [train_data[i: i+batch_size] for i in range(0, len(train_data), batch_size)][:-1]\n",
    "\n",
    "    for b in range(len(batchs_data) -1):\n",
    "        times = tf.Variable(batchs_data[b][:, 0, :3], tf.float64)\n",
    "        data_batch = tf.Variable(batchs_data[b][:, 0, 3:], tf.float64)\n",
    "        next_batch = tf.Variable(batchs_data[b +1][:, 0, 3:], tf.float64)\n",
    "\n",
    "        loss = train_step(times, data_batch, next_batch, len_predict_days)\n",
    "        print(loss)\n",
    "\n",
    "\n",
    "train_make_predict()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-26T17:41:01.491546800Z",
     "start_time": "2023-07-26T17:39:39.214556Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#########################################################################################################"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def print_weather_predict(ai, len_predict_days=3):\n",
    "    print(f\"Prediction for the next {len_predict_days} days:\\t\\t\\t\",\n",
    "          f\"(Temperature, Pressure, Humidity, Cloud, Raininess)\")\n",
    "\n",
    "    # Самое последнее - самое свежее\n",
    "    # Также сразу подаём только amount_available_context данных для прогноза\n",
    "    fresh_data = np.reshape(np.array(get_fresh_data(len_predict_days * 24)),\n",
    "                            (len_predict_days * 24, 8))[::-1]\n",
    "    times = fresh_data[:, :3].tolist()\n",
    "    predicts_history = fresh_data[:, 3:].tolist()\n",
    "\n",
    "\n",
    "    for _ in range(0, len_predict_days*24):\n",
    "        # Делаем прогноз по всей истории, а потом отбираем один прогноз, относящееся к последней записи\n",
    "        # preds_on_preds = ai.predict(np.array( [[t + p] for t, p in zip(times, predicts_history)] ), verbose=False)\n",
    "        # ai_ans = np.reshape(np.array(preds_on_preds)[:, -1], (5))\n",
    "        preds_on_preds = ai.predict(np.array([[times[-1] + predicts_history[-1]]]), verbose=False)\n",
    "        ai_ans = np.reshape(np.array(preds_on_preds), (5))\n",
    "        ai_ans = normalize(ai_ans, True)\n",
    "\n",
    "        # predicts_history.append( (ai_ans + np.array(predicts_history[-1])).tolist() )\n",
    "        predicts_history.append( (ai_ans + np.array(predicts_history)).tolist() )\n",
    "        # predicts_history = predicts_history[1:]\n",
    "\n",
    "        # Обновляем время\n",
    "        time = times[-1]\n",
    "        time[0] += 1/12                          # Увеличиваем часы\n",
    "        time[1] += 1/15.5 if time[0] >1 else 0   # Увеличиваем день\n",
    "        time[2] += 1/6    if time[1] >1 else 0   # Увеличиваем месяц\n",
    "\n",
    "        # Следим, чтобы зачения не выходили за границы\n",
    "        time = [-1 if i>1 else i for i in time]\n",
    "\n",
    "        times.append(time)\n",
    "\n",
    "\n",
    "        # Выводим прогноз\n",
    "        time_for_human = [norm_hours(time[0], True),\n",
    "                          norm_day(time[1], True),\n",
    "                          norm_month(time[2], True)]\n",
    "\n",
    "        pred_weather_for_human = conv_ai_ans_for_human(predicts_history[-1])\n",
    "        print(f\"{'{:02}'.format(time_for_human[1])}.{'{:02}'.format(time_for_human[2])}\",\n",
    "              f\"{'{:02}'.format(time_for_human[0])}:00:\\t\",\n",
    "              pred_weather_for_human[0], \"℃\\t\",\n",
    "              pred_weather_for_human[1], \"mmHg\\t\",\n",
    "              pred_weather_for_human[2], \"%\\t\",\n",
    "              pred_weather_for_human[3], \"%\\t\",\n",
    "              conv_rain_to_words(pred_weather_for_human[4]),\n",
    "              )\n",
    "    print(\"\\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print_weather_predict(ai, 3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
